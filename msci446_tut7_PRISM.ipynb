{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Mining:- Rule Mining:- PRISM:- Rule Based Classifier\n",
        "\n",
        "### Publications on PRISM:\n",
        "\n",
        "[Cendrowska, J. (1987). PRISM: an Algorithm for Inducing Modular Rules. International Journal of Man-Machine Studies, 27, pp. 349-370](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1987-Cendrowska-IJMMS.pdf)\n",
        "\n",
        "Bramer, M.A. (2000). Automatic Induction of Classification Rules from Examples Using N-Prism. In: Research and Development in Intelligent Systems XVI. Springer-Verlag, pp. 99-121\n",
        "\n",
        "Bramer, M.A. (2002). An Information-Theoretic Approach to the Pre-pruning of Classification Rules. Proceedings of the IFIP World Computer Congress, Montreal, 2002.\n"
      ],
      "metadata": {
        "id": "F6Xci-Xx1Ay3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Number of Instances: 24\n",
        "\n",
        "Number of Attributes: 4 (all nominal)\n",
        "\n",
        "Attribute Information:<br>\n",
        "     -- 3 Classes<br>\n",
        "      1 : the patient should be fitted with hard contact lenses,<br>\n",
        "      2 : the patient should be fitted with soft contact lenses,<br>\n",
        "      3 : the patient should not be fitted with contact lenses.<br>\n",
        " \n",
        "     1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
        "     2. spectacle prescription:  (1) myope, (2) hypermetrope\n",
        "     3. astigmatic:     (1) no, (2) yes\n",
        "     4. tear production rate:  (1) reduced, (2) normal\n",
        " \n",
        "Number of Missing Attribute Values:   0\n",
        " \n",
        "Class Distribution:<br>\n",
        "    1. hard contact lenses: 4<br>\n",
        "    2. soft contact lenses: 5<br>\n",
        "    3. no contact lenses: 15<br>\n",
        "\n",
        "relation contact-lenses\n",
        "\n",
        "attribute age \t\t\t{young, pre-presbyopic, presbyopic} <br>\n",
        "attribute spectacle-prescrip\t{myope, hypermetrope}<br>\n",
        "attribute astigmatism\t\t{no, yes}<br>\n",
        "attribute tear-prod-rate\t{reduced, normal}<br>\n",
        "attribute contact-lenses\t{soft, hard, none}\n"
      ],
      "metadata": {
        "id": "vt4jakDj2ByQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRISM Algorithm:"
      ],
      "metadata": {
        "id": "OL_oteMK4eJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "\n",
        "\n",
        "class RuleBasedClassifiers():\n",
        "\n",
        "    X_train = [];\n",
        "    X_test = [];\n",
        "    y_train = [];\n",
        "    y_test = [];\n",
        "\n",
        "    n_samples = 0;\n",
        "    n_features = 0;\n",
        "\n",
        "    csv_datasets = [\"/content/contact_lenses.csv\"]; # we can load multiple datasets as well\n",
        "\n",
        "    csv_datasets_col_names = [['Age','SpectaclePrescrip','Astigmatism','TearProdRate','ContactLens']];  # update here if loading multiple datasets\n",
        "\n",
        "####################################\n",
        "\n",
        "    def repair_continuous_attributes(self, dataset, features):\n",
        "\n",
        "        self.n_samples = dataset.shape[0];\n",
        "        self.n_features = dataset.shape[1] - 1;\n",
        "\n",
        "        for feat in features:\n",
        "            if dataset[feat].dtype == np.float64:\n",
        "                dataset[feat] = dataset[feat].astype(int);\n",
        "\n",
        "    def csv_processor(self, csv_path, feature_names):\n",
        "\n",
        "        dataset = pd.read_csv(csv_path);\n",
        "        dataset.columns = feature_names;\n",
        "        return dataset;\n",
        "\n",
        "\n",
        "    def fix_dataset_missing_values(self,dataset):\n",
        "\n",
        "        for column in dataset.columns:\n",
        "            dataset[column] = dataset[column].replace('?',np.NaN);\n",
        "            dataset[column] = dataset[column].fillna(dataset[column].value_counts().index[0]);\n",
        "\n",
        "    def build_learning_sets(self,dataset,class_attr,train_size):\n",
        "\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True);\n",
        "        n_train = int(self.n_samples*train_size);\n",
        "        n_test = self.n_samples - n_train;\n",
        "\n",
        "        dataset_ = dataset.copy(deep=True);\n",
        "        self.fix_dataset_missing_values(dataset_);\n",
        "\n",
        "        #print(dataset_);\n",
        "\n",
        "        #raw_input(\"STOPP!!!\");\n",
        "\n",
        "        self.y_train = dataset_.loc[0:n_train,class_attr].copy(deep=True);\n",
        "        self.y_test = dataset_.loc[n_train+1:self.n_samples,class_attr].copy(deep=True);\n",
        "\n",
        "        dataset_ = dataset_.drop(class_attr,axis=1);\n",
        "\n",
        "        self.X_train = dataset_.loc[0:n_train].copy(deep=True);\n",
        "        self.X_test = dataset_.loc[n_train+1:self.n_samples].copy(deep=True);\n",
        "\n",
        "    def display_data_info(self, dataset):\n",
        "\n",
        "        print(\"\\n1. Number of samples: \" + str(self.n_samples));\n",
        "        print(\"\\n2. Number of features: \" + str(self.n_features));\n",
        "        # print(\"\\n3. Feature types:\");\n",
        "        # print(dataset.dtypes);\n",
        "        # print(\"\\n4. Data:\");\n",
        "        # print(dataset);\n",
        "        # print(\"\\n5. Training sets:\");\n",
        "        # print(self.X_train);\n",
        "        # print(self.y_train);\n",
        "        # print(\"\\n6. Testing sets:\");\n",
        "        # print(self.X_test);\n",
        "        # print(self.y_test);\n",
        "\n",
        "########################################\n",
        "   \n",
        "    def data_preprocessing(self):\n",
        "\n",
        "        #print('A) ::Processing CSV files::');\n",
        "        dataset = self.csv_processor(self.csv_datasets[0],self.csv_datasets_col_names[0]); # update here if loading multiple datasets and want to analyze a specific one\n",
        "\n",
        "        #print('B) ::Repairing continuous attributes in Dataset::');\n",
        "        self.repair_continuous_attributes(dataset,dataset.columns);\n",
        "\n",
        "        #print('C) ::Building train/test sets::');\n",
        "        self.build_learning_sets(dataset,dataset.columns[-1],1.0);\n",
        "\n",
        "        #print('D) ::Dataset Information::');\n",
        "        #self.display_data_info(dataset);\n",
        "\n",
        "############################################\n",
        "\n",
        "    def PRISM(self):\n",
        "\n",
        "        # print(\"\\n::: DATASET X,Y:::\");\n",
        "        # print(self.X_train);\n",
        "        # print(self.y_train);\n",
        "\n",
        "        # print(\"\\n:::PRISM Algorithm:::\");\n",
        "\n",
        "        prism_rule_set = [];\n",
        "        for label in set(self.y_train):\n",
        "\n",
        "            #print(\"<<<<<<<<< CURRENT LABEL: \"+str(label)+\">>>>>>>>>>\");\n",
        "\n",
        "            instances = [i for i, val in enumerate(self.y_train) if val == label];\n",
        "\n",
        "            while instances:\n",
        "                rule = [];\n",
        "                X_train_ = self.X_train.copy(deep=True);\n",
        "                instances_covered = [];\n",
        "                perfect_rule = False;\n",
        "\n",
        "                # print(\" ******** WHILE PERFECT RULE? ********* \");\n",
        "                # print(\"\\n\");\n",
        "\n",
        "                rule_precision = 0.0;\n",
        "                rule_coverage = 0.0;\n",
        "\n",
        "                while perfect_rule == False and len(rule) < self.n_features+1:\n",
        "                    optimal_selector = [(\"\",\"\")];\n",
        "                    optimal_selector_prec = [0.0,0.0,0.0];\n",
        "                    instances_covered = [];\n",
        "\n",
        "                    # print(\"^^^^^^^^ INSTANCES TO FIT ^^^^^^^^^\");\n",
        "                    # print(instances);\n",
        "                    # print(\"\\n\");\n",
        "\n",
        "\n",
        "                    # print(\" %%%%%%%% PREVIOUS OPT SELECTOR %%%%%%% \");\n",
        "                    # print(optimal_selector);\n",
        "                    # print(optimal_selector_prec);\n",
        "                    # print(\"\\n\");\n",
        "\n",
        "\n",
        "                    for attribute in X_train_.columns:\n",
        "                        attr_column = X_train_.loc[:,attribute];\n",
        "\n",
        "                        for attr_value in set(attr_column):\n",
        "\n",
        "                            total_attr_values_instances = np.asarray(attr_column[(attr_column == attr_value)].index);\n",
        "                            total_matches = len(total_attr_values_instances);\n",
        "                            #print(\"::::TOTALS::: size = \"+str(total_matches));\n",
        "                            #print(total_attr_values_instances);\n",
        "\n",
        "\n",
        "                            positive_attr_values_instances = list(set(total_attr_values_instances) & set(instances));\n",
        "                            positive_matches = len(positive_attr_values_instances);\n",
        "                            #print(\"::::POSITIVES::: size = \"+str(positive_matches));\n",
        "                            #print(positive_attr_values_instances);\n",
        "\n",
        "                            #Computing Precision of the rule (by considering a filtered dataset instances by the selectors in rule)\n",
        "                            precision = (1.0 * positive_matches) / total_matches;\n",
        "\n",
        "                            #Computing Coverage of the rule (considering all the instances coverd by the rule divided by all instances in dataset)\n",
        "                            coverage = (1.0 * positive_matches) / self.n_samples;\n",
        "\n",
        "\n",
        "\n",
        "                            if precision > optimal_selector_prec[2]:\n",
        "                                optimal_selector = (attribute,attr_value);\n",
        "                                optimal_selector_prec[0] = positive_matches;\n",
        "                                optimal_selector_prec[1] = total_matches;\n",
        "                                optimal_selector_prec[2] = precision;\n",
        "                                rule_precision = precision;\n",
        "                                rule_coverage = coverage;\n",
        "                                instances_covered = positive_attr_values_instances;\n",
        "\n",
        "                            elif precision == optimal_selector_prec[2] and positive_matches > optimal_selector_prec[0]:\n",
        "                                optimal_selector = (attribute, attr_value);\n",
        "                                optimal_selector_prec[0] = positive_matches;\n",
        "                                optimal_selector_prec[1] = total_matches;\n",
        "                                optimal_selector_prec[2] = precision;\n",
        "                                instances_covered = positive_attr_values_instances;\n",
        "                                rule_precision = precision;\n",
        "                                rule_coverage = coverage;\n",
        "\n",
        "                    # print(\" %%%%%%%% UPDATED OPT SELECTOR ? %%%%%%% \");\n",
        "                    # print(optimal_selector);\n",
        "                    # print(optimal_selector_prec);\n",
        "                    # print(\"\\n\");\n",
        "\n",
        "\n",
        "\n",
        "                    if optimal_selector_prec[2] > 0.0 and optimal_selector_prec[2] < 1.0:\n",
        "\n",
        "                        #print(\" ***** AFTER CHECK ALL ATTR-VALS PAIRS MY RULE IS NOT PERFECT BUT (PREC > 0) ***** \");\n",
        "\n",
        "                        #print(X_train_);\n",
        "                        #print(np.asarray(X_train_.index));\n",
        "\n",
        "                        rule.append(optimal_selector);\n",
        "                        selector = rule[-1]\n",
        "\n",
        "                        #print(\"FILTER SELECTOR::\");\n",
        "                        #print(selector);\n",
        "\n",
        "                        #print(\"ACCESSING TO SELECTOR ATTR TO OBTAIN FILTER INDEXES:::\");\n",
        "                        filtered_rows = np.asarray(X_train_[(X_train_[selector[0]] != selector[1])].index);\n",
        "                        #print(filtered_rows);\n",
        "\n",
        "                        #print(\"FILTERING DATASET BY CUMULATIVE RULE OF SELECTORS::\");\n",
        "\n",
        "                        X_train_ = X_train_.drop(filtered_rows).copy(deep=True);\n",
        "                        X_train_ = X_train_.drop(selector[0], axis=1);\n",
        "\n",
        "                        #print(\"IF THERE ARE NO MORE ATTRIBUTES TO COMPOSE THE RULE:::\");\n",
        "\n",
        "                        if len(X_train_.columns) == 0:\n",
        "                            perfect_rule = True;\n",
        "                            continue;\n",
        "\n",
        "                        #print(\" %%%%%%%%%% X_train_ FILTERED BY CURRENT COMPOSED RULE %%%%%%%%%%%\");\n",
        "                        #print(X_train_);\n",
        "                        #print(\"\\n\");\n",
        "\n",
        "\n",
        "                    elif optimal_selector_prec[2] == 1.0:\n",
        "\n",
        "                        #print(\" ***** AFTER CHECK ALL ATTR-VALS PAIRS MY RULE IS PERFECT!!! ***** \");\n",
        "                        rule.append(optimal_selector);\n",
        "                        perfect_rule = True;\n",
        "                        continue;\n",
        "\n",
        "                    elif optimal_selector_prec[2] == 0.0:\n",
        "                        raw_input(\"....... UNSUAL CASE .......\");\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # print(\"^^^^^^^^ INSTANCES COVERED ^^^^^^^^^\");\n",
        "                # print(instances_covered);\n",
        "                # print(\"\\n\");\n",
        "\n",
        "                instances = list(set(instances) - set(instances_covered));\n",
        "\n",
        "                # print(\"^^^^^^^^ INSTANCES REMAINING ^^^^^^^^^\");\n",
        "                # print(instances);\n",
        "\n",
        "                rule.append(label);\n",
        "                rule.append([rule_precision,rule_coverage]);\n",
        "\n",
        "                print(\"++++++++ RULE FOUND +++++++++\");\n",
        "                metrics = rule[-1];\n",
        "\n",
        "                print(\"Rule:\");\n",
        "                print(rule);\n",
        "                print(\"Rule-Precision: \"+str(metrics[0]));\n",
        "                print(\"Rule-Coverage: \"+str(metrics[1]));\n",
        "                print(\"\\n\");\n",
        "\n",
        "                prism_rule_set.append(rule);\n",
        "\n",
        "        return prism_rule_set;\n",
        "\n",
        "\n",
        "###################################\n",
        "\n",
        "RBC = RuleBasedClassifiers();\n",
        "\n",
        "RBC.data_preprocessing();\n",
        "rule_set = RBC.PRISM();\n",
        "\n",
        "print(\"%%%%%%%%%%%%%%%%% FINAL PRISM RULE SET %%%%%%%%%%%%%%%%%\");\n",
        "\n",
        "print(\"\\n\");\n",
        "for prism_rule in rule_set:\n",
        "    print(prism_rule);\n"
      ],
      "metadata": {
        "id": "LzQdQXet_wht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8787490d-2cc3-471d-fcf8-f0005c9031c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('Astigmatism', 'yes'), ('TearProdRate', 'normal'), ('SpectaclePrescrip', 'myope'), 'hard', [1.0, 0.125]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.125\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('Age', 'young'), ('SpectaclePrescrip', 'hypermetrope'), ('Astigmatism', 'yes'), ('TearProdRate', 'normal'), 'hard', [1.0, 0.041666666666666664]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.041666666666666664\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('Astigmatism', 'no'), ('TearProdRate', 'normal'), ('SpectaclePrescrip', 'hypermetrope'), 'soft', [1.0, 0.125]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.125\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('SpectaclePrescrip', 'myope'), ('Astigmatism', 'no'), ('TearProdRate', 'normal'), ('Age', 'pre-presbyopic'), 'soft', [1.0, 0.041666666666666664]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.041666666666666664\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('Age', 'young'), ('SpectaclePrescrip', 'myope'), ('Astigmatism', 'no'), ('TearProdRate', 'normal'), 'soft', [1.0, 0.041666666666666664]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.041666666666666664\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('TearProdRate', 'reduced'), 'none', [1.0, 0.5]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.5\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('TearProdRate', 'normal'), ('Age', 'presbyopic'), ('SpectaclePrescrip', 'hypermetrope'), ('Astigmatism', 'yes'), 'none', [1.0, 0.041666666666666664]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.041666666666666664\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('TearProdRate', 'normal'), ('Age', 'pre-presbyopic'), ('SpectaclePrescrip', 'hypermetrope'), ('Astigmatism', 'yes'), 'none', [1.0, 0.041666666666666664]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.041666666666666664\n",
            "\n",
            "\n",
            "++++++++ RULE FOUND +++++++++\n",
            "Rule:\n",
            "[('Age', 'presbyopic'), ('SpectaclePrescrip', 'myope'), ('Astigmatism', 'no'), ('TearProdRate', 'normal'), 'none', [1.0, 0.041666666666666664]]\n",
            "Rule-Precision: 1.0\n",
            "Rule-Coverage: 0.041666666666666664\n",
            "\n",
            "\n",
            "%%%%%%%%%%%%%%%%% FINAL PRISM RULE SET %%%%%%%%%%%%%%%%%\n",
            "\n",
            "\n",
            "[('Astigmatism', 'yes'), ('TearProdRate', 'normal'), ('SpectaclePrescrip', 'myope'), 'hard', [1.0, 0.125]]\n",
            "[('Age', 'young'), ('SpectaclePrescrip', 'hypermetrope'), ('Astigmatism', 'yes'), ('TearProdRate', 'normal'), 'hard', [1.0, 0.041666666666666664]]\n",
            "[('Astigmatism', 'no'), ('TearProdRate', 'normal'), ('SpectaclePrescrip', 'hypermetrope'), 'soft', [1.0, 0.125]]\n",
            "[('SpectaclePrescrip', 'myope'), ('Astigmatism', 'no'), ('TearProdRate', 'normal'), ('Age', 'pre-presbyopic'), 'soft', [1.0, 0.041666666666666664]]\n",
            "[('Age', 'young'), ('SpectaclePrescrip', 'myope'), ('Astigmatism', 'no'), ('TearProdRate', 'normal'), 'soft', [1.0, 0.041666666666666664]]\n",
            "[('TearProdRate', 'reduced'), 'none', [1.0, 0.5]]\n",
            "[('TearProdRate', 'normal'), ('Age', 'presbyopic'), ('SpectaclePrescrip', 'hypermetrope'), ('Astigmatism', 'yes'), 'none', [1.0, 0.041666666666666664]]\n",
            "[('TearProdRate', 'normal'), ('Age', 'pre-presbyopic'), ('SpectaclePrescrip', 'hypermetrope'), ('Astigmatism', 'yes'), 'none', [1.0, 0.041666666666666664]]\n",
            "[('Age', 'presbyopic'), ('SpectaclePrescrip', 'myope'), ('Astigmatism', 'no'), ('TearProdRate', 'normal'), 'none', [1.0, 0.041666666666666664]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision and Coverage of Rule:<br>\n",
        "Computing Precision of the rule (by considering a filtered dataset instances by the selectors in rule)<br>\n",
        "precision = (1.0 * positive_matches) / total_matches\n",
        "\n",
        "Computing Coverage of the rule (considering all the instances coverd by the rule divided by all instances in dataset)<br>\n",
        "coverage = (1.0 * positive_matches) / self.n_samples"
      ],
      "metadata": {
        "id": "zOhrUrzS-5gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Further reading material: <br>\n",
        "\n",
        "[Paper: Data mining- Rule-Based Classification](http://webpages.iust.ac.ir/yaghini/Courses/Data_Mining_882/DM_05_04_Rule-Based%20Classification.pdf)\n",
        "\n",
        "[Paper: Building a classifier employing PRISM\n",
        "algorithm with Fuzzy logic](https://aircconline.com/ijdkp/V7N2/7217ijdkp04.pdf)\n",
        "\n",
        "[Paper: Top-Down Induction of Decision Trees (TDIDT) vs PRISM vs Information Entropy Based Rule Generation (IEBRG)](https://www.researchgate.net/publication/281447983_Induction_of_Modular_Classification_Rules_by_Information_Entropy_Based_Rule_Generation) <br>\n",
        "\n",
        "[PRISM from Math Perspective- Lecture notes- CS 831- U of Regina](http://www2.cs.uregina.ca/~deng200x/PRISM_PPT.pdf)\n",
        "\n",
        "[PRISM Explanation YouTube Link](https://www.youtube.com/watch?v=gQVB--_mxXY)\n",
        "\n"
      ],
      "metadata": {
        "id": "o672k6CnCYTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <> Reference:<br>\n",
        "https://github.com/justpablo/PRISM-Rule-Based-Classification/blob/master/RuleBasedClassifiers.py<br>\n",
        "A rule-based classfier, featuring a non-ordered technique with a non-incremental and selector-based learning style, comprising a general-especific learning approach assuring a 100% precison of classification.<br>\n",
        "\n",
        "\n",
        "Encoded Dataset also available at https://archive.ics.uci.edu/ml/datasets/lenses"
      ],
      "metadata": {
        "id": "ZzHWEU2XPTKg"
      }
    }
  ]
}